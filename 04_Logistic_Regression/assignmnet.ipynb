{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning\n",
    "## Logistic Regression \n",
    "Logistic Regression is a fundamental supervised learning algorithm used for binary classification tasks. Despite its name, it's used for classification, not regression. Logistic Regression models the probability that a given input belongs to a particular class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation Steps\n",
    "1. Data Pre-processing\n",
    "2. Parameter Initialization\n",
    "3. Forward Propagation:\n",
    "4. Cost Calculation\n",
    "5. Backward Propagation\n",
    "6. Parameter Update\n",
    "7. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import libraries we've already learnt and will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Pre-Processing\n",
    "The dataset contains information that may be used for diagnosing or predicting the presence of heart disease in individuals. It comprises various clinical and demographic features that are commonly considered in cardiovascular health assessment. Understanding and analyzing these features can aid in developing predictive models or understanding the factors associated with heart disease.  \n",
    "The first 13 fields are \n",
    "- Age\n",
    "- Sex\n",
    "- Chest Pain Type (cp)\n",
    "- Resting Blood Pressure (trestbps)\n",
    "- Serum Cholesterol Level (chol)\n",
    "- Fasting Blood Sugar (fbs)\n",
    "- Resting Electrocardiographic Results (restecg)\n",
    "- Maximum Heart Rate Achieved (thalach)\n",
    "- Exercise-Induced Angina (exang)\n",
    "- ST Depression Induced by Exercise Relative to Rest (oldpeak)\n",
    "- Slope of the Peak Exercise ST Segment (slope)\n",
    "- Number of Major Vessels Colored by Fluoroscopy (ca)\n",
    "- Thalassemia (thal)  \n",
    "\n",
    "And the 14th field is 'target' and is either 0 or 1. The \"target\" field refers to the presence of heart disease in the patient. It is integer valued 0 = no disease and 1 = disease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import data from the csv file provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_raw = np.genfromtxt(\"./dataset/heart.csv\", dtype=\"str\", delimiter=\",\")\n",
    "print(dataset_raw.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset have headers that we don't need for logistic regression model. That is there for us to understand what the values denote  \n",
    "We seperate the headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = dataset_raw[0, :]\n",
    "print(headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we get rest of the numerical data and cast them as `float` data type instead of `string`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_raw[1:, :]\n",
    "dataset = dataset.astype(float)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, the first 13 columns represent the features, while the 14th column indicates whether the individual has the disease or not based on those features. Here we seperate the dataset into X (feature vector) and Y (output vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[:, :13]\n",
    "Y = dataset[:, 13]\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape of X here is $(m, n_x)$, but we want it to have $shape = (n_x, m)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.T\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looks good\n",
    "Finally, we have 1025 examples of 13 features and 1 output  \n",
    "From the notation you have studied till now  \n",
    "$$\n",
    "\\begin{align*}\n",
    "n_x &= 13 \\\\\n",
    "n_y &= 1 \\\\\n",
    "m &= 1025\n",
    "\\end{align*}\n",
    "$$\n",
    "Now we can proceed to making our logistic regression model and train our model  \n",
    "But, we have one problem to deal with. How would we know if our model is doing good and if it is, how good is it?  \n",
    "That is why from the 1025 examples we have, we'll keep some data aside and use it to test our model's prediction  \n",
    "Let's keep 80% of the data for training and 20% of the data for testing our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get index to split data in 80:20 ratio\n",
    "index = int(0.8 * X.shape[1])\n",
    "\n",
    "# split the data\n",
    "X_train = X[:, :index]\n",
    "X_test = X[:, index:]\n",
    "\n",
    "Y_train = Y[:index]\n",
    "Y_test = Y[index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print shapes for our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape\", X_train.shape)\n",
    "print(\"Y_train shape\", Y_train.shape)\n",
    "print(\"Number of training examples =\", Y_train.shape[0])\n",
    "print(\"-\"*40)\n",
    "print(\"X_test shape\", X_test.shape)\n",
    "print(\"Y_test shape\", Y_test.shape)\n",
    "print(\"Number of testing examples =\", Y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression \n",
    "#### Forward Propagation\n",
    "$$\n",
    "Z = W^T X + b  \\\\\n",
    "A = sigmoid(X) \\\\\n",
    "$$\n",
    "#### Calculate Cost\n",
    "$$\n",
    "J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})\n",
    "$$\n",
    "#### Backward Propagation\n",
    "$$ \\partial W = \\frac{\\partial J}{\\partial W} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n",
    "$$ \\partial b = \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$\n",
    "#### Parameter Updation\n",
    "$$ W = W - \\alpha \\text{ } \\partial W $$\n",
    "$$ b = b - \\alpha \\text{ } \\partial b $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initializing parameters\n",
    "**Assignment**: Complete the function for parameter initialization in the cell below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(num_features):\n",
    "  \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
    "\n",
    "    Returns:\n",
    "    W -- initialized vector of shape (num_features, 1)\n",
    "    b -- initialized scalar (corresponds to the bias)\n",
    "  \"\"\"\n",
    "\n",
    "  W = np.zeroes(num_features,1)\n",
    "  b = 0\n",
    "\n",
    "  return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Helper Function**: Sigmoid\n",
    "**Assignment**: Complete the function for calculating sigmoid in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "  \"\"\"\n",
    "    Compute the sigmoid of x\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "  \"\"\"\n",
    "  \n",
    "  s = 1/(1+np.exp(-x)\n",
    "\n",
    "  return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Forward Propagation\n",
    "$$\n",
    "Z = W^T X + b  \\\\\n",
    "A = sigmoid(X) \\\\\n",
    "$$\n",
    "**Assignment**: Complete the function implementing forward propagation in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(W, b, X):\n",
    "  \"\"\"\n",
    "    Compute forward propagation\n",
    "\n",
    "    Return:\n",
    "    A -- activation\n",
    "  \"\"\"\n",
    "\n",
    "  # forward propagation\n",
    "  Z = None\n",
    "  A = None\n",
    "  \n",
    "  return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calculate Cost\n",
    "$$\n",
    "J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})\n",
    "$$\n",
    "**Assignment**: Complete the function to calculate cost in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(A, Y):\n",
    "  \"\"\"\n",
    "    Calculate cross entropy loss between calculated values (A) and actual values (Y)\n",
    "\n",
    "    Return:\n",
    "    cost -- cost calculated\n",
    "  \"\"\"\n",
    "\n",
    "  # get number of examples\n",
    "  m = None\n",
    "\n",
    "  # calculate cost\n",
    "  cost = None\n",
    "\n",
    "  # this will remove any useless dimensions from cost\n",
    "  # not doing this might give us an array instead of a single value\n",
    "  cost = np.squeeze(cost)\n",
    "\n",
    "  return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Backward Propagation\n",
    "$$ \\partial W = \\frac{\\partial J}{\\partial W} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n",
    "$$ \\partial b = \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$\n",
    "**Assignment**: Complete the function to compute gradients in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop(A, X, Y):\n",
    "  \"\"\"\n",
    "    Calculate gradients dW and db\n",
    "\n",
    "    Return:\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "  \"\"\"\n",
    "\n",
    "  # get number of examples\n",
    "  m = None\n",
    "\n",
    "  # calculate gradients\n",
    "  dW = None\n",
    "  db = None\n",
    "\n",
    "  return dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Parameter Updation\n",
    "$$ W = W - \\alpha \\text{ } \\partial W $$\n",
    "$$ b = b - \\alpha \\text{ } \\partial b $$\n",
    "**Assignment**: Complete the function to update parameters in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(W, b, dW, db, learning_rate):\n",
    "  \"\"\"\n",
    "    Update params W and b from their gradients\n",
    "\n",
    "    Return:\n",
    "    W -- updated W\n",
    "    b -- updated b\n",
    "  \"\"\"\n",
    "\n",
    "  W = None\n",
    "  b = None\n",
    "\n",
    "  return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our model's forward propagation return value calculated from sigmoid function, it lies between 0 and 1  \n",
    "We can say output should be 1 when A > 0.5 and 0 when A < 0.5  \n",
    "Let's implement a function that will do this for us in vectorized way\n",
    "### Predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(W, b, X):\n",
    "  A = forward_prop(W, b, X)\n",
    "  Y_pred = (A>=0.5)*1.0\n",
    "  return Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's compile all these function to implement training loop for our model\n",
    "You don't need to write any math here, just put all the functions you've written above in the right sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, num_iterations=10000, learning_rate=0.0001, print_cost=True):\n",
    "  # initialize parameters\n",
    "  W, b = None\n",
    "\n",
    "  # let's keep track of our cost to see how our model\n",
    "  # reduces cost after every few iteration\n",
    "  costs = []\n",
    "\n",
    "  for i in range(num_iterations):\n",
    "\n",
    "    # forward propagation\n",
    "    A = None\n",
    "\n",
    "    # calculate cost\n",
    "    cost = None\n",
    "\n",
    "    # backward propagation\n",
    "    dW, db = None\n",
    "\n",
    "    # parameter updation\n",
    "    W, b = None\n",
    "\n",
    "    # store cost after every few iterations\n",
    "    if i%100 == 0:\n",
    "      costs.append(cost)\n",
    "\n",
    "    # print cost after every few iterations\n",
    "    if print_cost and i%100 == 0:\n",
    "      print(f\"Cost after {i+1} iteration : {cost}\")\n",
    "    \n",
    "  return W, b, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let the model train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, b, costs = train(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot costs to see how our model was converging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(costs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Model Evaluation\n",
    "Let's check accuracy on train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train accuracy: {} %\".format(100 - np.mean(np.abs(predict(W, b, X_train) - Y_train)) * 100))\n",
    "print(\"Test accuracy: {} %\".format(100 - np.mean(np.abs(predict(W, b, X_test) - Y_test)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Increasing the number of iterations can improve training accuracy, but there's a risk of overfitting. Overfitting occurs when the model learns to fit the training data too closely, capturing noise or irrelevant patterns that don't generalize well to unseen data. In such cases, while the training accuracy continues to increase, the test accuracy may plateau or even decrease.\n",
    "\n",
    "To address overfitting, techniques like regularization and model complexity reduction are employed. Additionally, the transition to deep neural networks offers more sophisticated methods for improving accuracies, even in the presence of overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
